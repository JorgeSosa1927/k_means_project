# K-Means Clustering with Parallelization

## Overview

This project implements the K-Means clustering algorithm in both sequential and parallel versions. The dataset is scaled to 70,000 samples for testing purposes. Performance benchmarking is performed to compare the execution times of the two implementations, and the speedup is visualized.

---

## Directory Structure

```
k_means_project/
├── benchmarks/               # Folder for datasets
│   └── dataset.csv           # Scaled dataset (generated by `dataset_generator.py`)
├── src/                      # Source code folder
│   ├── dataset_generator.py  # Script to generate the dataset
│   ├── kmeans_seq.py         # Sequential K-Means implementation
│   ├── kmeans_parallel.py    # Parallel K-Means implementation
│   ├── plot_speedup.py       # Speedup visualization script
├── README.md                 # Project documentation
├── requirements.txt          # Python dependencies
```

---

## Prerequisites

- Python 3.8+
- Pip package manager

Install dependencies using the provided `requirements.txt` file:
```bash
pip install -r requirements.txt
```

---

## Usage

### Step 1: Generate the Dataset
Run the `dataset_generator.py` script to create a scaled dataset:
```bash
python src/dataset_generator.py
```
The dataset will be saved in the `benchmarks/` folder as `dataset.csv`.

### Step 2: Run Sequential K-Means
Run the sequential implementation:
```bash
python src/kmeans_seq.py
```
This will output the execution time for the sequential implementation.

### Step 3: Run Parallel K-Means
Run the parallel implementation:
```bash
python src/kmeans_parallel.py
```
This will output the execution time for the parallel implementation.

### Step 4: Visualize Speedup
After collecting the execution times, update the `plot_speedup.py` script with the actual values and run:
```bash
python src/plot_speedup.py
```
This will generate a graph showing the speedup versus the number of threads.

---

## Expected Results

- For larger datasets, the parallel implementation should perform better than the sequential implementation.
- The speedup depends on the number of threads and the size of the dataset.

---

## Dependencies

The following Python libraries are required:
- `numpy`: For numerical computations.
- `pandas`: For dataset manipulation.
- `scikit-learn`: For dataset generation.
- `matplotlib`: For speedup visualization.
- `joblib`: For parallel computing.

Install them via:
```bash
pip install -r requirements.txt
```

---

## Known Issues

- For small datasets, the overhead of parallelization may outweigh the benefits.
- Ensure your system has sufficient CPU cores for the parallel implementation to show improvements.

---

## License

This project is open-source and licensed under the MIT License.